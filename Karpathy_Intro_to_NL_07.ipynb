{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will built a character-level Generative Pre-Trained Transformer (GPT) network.\n",
    "# The network should generate infinite shakespeare :P\n",
    "# We do this, similar to nevermore, by taking in a context of characters and predicting the next character (or \"token\")\n",
    "# Therefore, by learning the idiosyncracies of human speech, in this case of all Shakespeare's works, we will learn which tokens are likely to follow for any given context based on probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tiny-shakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of dataset in characters: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "B\n",
      "[14]\n",
      "B\n",
      "hello there\n",
      "[46, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43]\n",
      "hello there\n"
     ]
    }
   ],
   "source": [
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "encode = lambda s: [stoi[character] for character in s] # encoder: take in a string and return list of integers\n",
    "decode = lambda l: \"\".join([itos[i] for i in l]) # decoder: take in a list of integers and return a string\n",
    "\n",
    "print(stoi[\"B\"])\n",
    "print(itos[14])\n",
    "print(encode(\"B\"))\n",
    "print(decode(encode(\"B\")))\n",
    "\n",
    "print(\"hello there\")\n",
    "print(encode(\"hello there\"))\n",
    "print(decode(encode(\"hello there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# so we start by encoding the entire dataset into tensors of tokens using pyTorch\n",
    "import torch\n",
    "data = torch.tensor(encode(text))\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # this is what the first 1000 characters of our dataset will look like to the GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation data\n",
    "train_idx = int(0.9 * len(data)) # first 90% train; remaining will be used as validation\n",
    "train_data = data[:train_idx]\n",
    "val_data = data[train_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8 # sometimes referred to as context_size\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When context is tensor([18]), target is 47\n",
      "When context is tensor([18, 47]), target is 56\n",
      "When context is tensor([18, 47, 56]), target is 57\n",
      "When context is tensor([18, 47, 56, 57]), target is 58\n",
      "When context is tensor([18, 47, 56, 57, 58]), target is 1\n",
      "When context is tensor([18, 47, 56, 57, 58,  1]), target is 15\n",
      "When context is tensor([18, 47, 56, 57, 58,  1, 15]), target is 47\n",
      "When context is tensor([18, 47, 56, 57, 58,  1, 15, 47]), target is 58\n"
     ]
    }
   ],
   "source": [
    "# Quick example to highlight the parallel training of transformers\n",
    "x = train_data[:block_size+1] # Why +1 to block_size? So that the full block/context can be used to predict as well by including the token that would come after the provided context\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    # target = x[t+1] same as target = y[t]\n",
    "    target = y[t]\n",
    "    print(f\"When context is {context}, target is {target}\")\n",
    "\n",
    "\n",
    "# Why are transformers so cool? As we can see, with a context or block size of 8, we will train 8 sequences in parallel!\n",
    "# Transformers are not optimizing sequentially (as RNNs are), but compute in parallel!\n",
    "# This is what allows for the extreme scaling of context size and therefore the explosion of training sizes, i.e. the entire web\n",
    "# But besides efficiency, this is also amazing since it automatically allows the transformer to see all possible context windows, i.e. very small sequences and really large blocks of text\n",
    "# So the transformer automatically learns to predict the next token for many different circumstances\n",
    "    \n",
    "# If we use a context / block size of 8, we do 8 computations in parallel\n",
    "# In order for this to work, we need to include the block_size + 1nth token as well\n",
    "# So that the full context, in this case 8 tokens, can be used as an individual example as well\n",
    "# were we to ommit the +1, we could only train on context up to the len(context) - 1nth element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "This torch.Size([4, 8]) array contains a total of 32 completely independent examples.\n",
      "Since each row has 8 samples and we provide 4 rows per training batch.\n",
      "          \n",
      "outputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "--------------------------------------------------\n",
      "when input is [24], target is 43\n",
      "when input is [24, 43], target is 58\n",
      "when input is [24, 43, 58], target is 5\n",
      "when input is [24, 43, 58, 5], target is 57\n",
      "when input is [24, 43, 58, 5, 57], target is 1\n",
      "when input is [24, 43, 58, 5, 57, 1], target is 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46], target is 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43], target is 39\n",
      "******************************\n",
      "when input is [44], target is 53\n",
      "when input is [44, 53], target is 56\n",
      "when input is [44, 53, 56], target is 1\n",
      "when input is [44, 53, 56, 1], target is 58\n",
      "when input is [44, 53, 56, 1, 58], target is 46\n",
      "when input is [44, 53, 56, 1, 58, 46], target is 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39], target is 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58], target is 1\n",
      "******************************\n",
      "when input is [52], target is 58\n",
      "when input is [52, 58], target is 1\n",
      "when input is [52, 58, 1], target is 58\n",
      "when input is [52, 58, 1, 58], target is 46\n",
      "when input is [52, 58, 1, 58, 46], target is 39\n",
      "when input is [52, 58, 1, 58, 46, 39], target is 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58], target is 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1], target is 46\n",
      "******************************\n",
      "when input is [25], target is 17\n",
      "when input is [25, 17], target is 27\n",
      "when input is [25, 17, 27], target is 10\n",
      "when input is [25, 17, 27, 10], target is 0\n",
      "when input is [25, 17, 27, 10, 0], target is 21\n",
      "when input is [25, 17, 27, 10, 0, 21], target is 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1], target is 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54], target is 39\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "# Why utilize batches?\n",
    "# To put it simply: To keep the GPUs busy.\n",
    "# GPUs are very good at parallel tasks; therefore, we want to divide our training set into independent chunks and run them in parallel on different areas of the GPUs.\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # What is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split ==\"train\" else val_data\n",
    "    # IMPORTANT NOTE: w/o -block_size, we would run into potential errors if we sample a random integer that is less than block_size steps away from our max index! Take care.\n",
    "    # This also enables our block_size to always be guaranteed a target value. Think about it.\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # generate random starting point indices for a sequence\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # extract the sequence from its random starting point\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # extract the sequence of targets by one-offsetting\n",
    "\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch(\"train\") # b for batch\n",
    "print(\"inputs:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "# Important:\n",
    "print(f\"This {xb.shape} array contains a total of {xb.shape[0] * xb.shape[1]} completely independent examples.\")\n",
    "print(f\"Since each row has {xb.shape[1]} samples and we provide {xb.shape[0]} rows per training batch.\")\n",
    "print(\" \"*10)\n",
    "print(\"outputs:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"-\"*50)\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()}, target is {target}\")\n",
    "    print(\"*\"*30)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start with the simplest language model - the Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(1337);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads of the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensors of integers\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C) <-- Batch, Time, Channel\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # instead of 4x8x65, we stretch out the array to 32x65\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the prediction\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last step\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # take next most likely character by sampling from distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.concat((idx, idx_next), dim=1) # (B, T+1)\n",
    "\n",
    "        return idx\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "# optimizer = torch.optim.SGD(m.parameters(), lr=1e-3) # we previously only used Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.672869920730591\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "for steps in range(1000):\n",
    "\n",
    "    # sample batch from data\n",
    "    xb, yb = get_batch(train_data)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wh;;SPif uslI'Tnc\n",
      "kwgOj$dhPWr,SV?hsusiKpgXXUh;Apmem sunESX&GT;TrJgkiF-oKbXCAA -botrngFCHOptto$\n",
      "\n",
      "pn$w-gHoi?wtd!\n",
      "wLU-IfSK'bAw :M.ZtOptXEQcL?hfaofqbPd?O:PnQQJMap$aypupIBfGJsZaI'ottllo..k$W$Akp?yl?ajKlzY!lx&QQLW? t,bXFkyhl-dmVsHeckhRl,jSClgjuk:3Iv\n",
      "?OqlrV;!Plxfzgy;;\n",
      "'mRjuBQ&xk!$\n",
      "h\n",
      "SiruDJgKuDny,S$ERf.?GSV-ivvKcOvi-nQGX&q-YQbm dEM?px;Akr-IENAc-wIWcd\n",
      "RFgXTpDUgM:CH.D&uo'IBT -\n",
      "j?wfy fFr.&fiqtRS.ZttxGh KG'd!rn$zoZqbocL&yIffBDWNGbo,Se,\n",
      "o.Fls,?,M?eZxHx,j?EV.mJiHqHnfF-wbQpa;P fawiF$-QbWv&f:CVDCBfano,b?$Esev.?\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(m.generate(idx, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn((B,T,C))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B,T,C)) # bow = bag of words\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t, C) # remember that python indexing works UP TO! so in the case of [b, :2], we extract the 0th and 1st elements, while [b, :1] would only extract UP TO the first element, so only the 0th element\n",
    "        xbow[b, t] = torch.mean(xprev, 0) # calculate average of previous elements in current batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c=\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.ones((3,3))\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"c=\")\n",
    "print(c)\n",
    "\n",
    "# Important learning/reminder: Matrix Multiplication using a Matrix of Ones is just summation!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c=\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# now, what would happen, if we were to use a triangular matrix\n",
    "# A square matrix is called lower triangular if all the entries above the main diagonal are zero\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones((3,3)))\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"c=\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# and what would happen, were we to normalize across the rows of our triangular matrix of 1s?\n",
    "# we enable efficient average calculation using matrix multiplication\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones((3,3)))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"c=\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "a before masking=\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "a after masking=\n",
      "tensor([[0., -inf, -inf],\n",
      "        [0., 0., -inf],\n",
      "        [0., 0., 0.]])\n",
      "a after softmax=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b=\n",
      "tensor([[0., 4.],\n",
      "        [0., 3.],\n",
      "        [8., 4.]])\n",
      "c=\n",
      "tensor([[0.0000, 4.0000],\n",
      "        [0.0000, 3.5000],\n",
      "        [2.6667, 3.6667]])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones(3,3)) # initialize seperate lower triangular mask\n",
    "print(\"mask=\")\n",
    "print(mask)\n",
    "\n",
    "a = torch.zeros((3,3)) # this is the weight matrix\n",
    "print(\"a before masking=\")\n",
    "print(a)\n",
    "a = a.masked_fill(mask == 0, float(\"-inf\"))\n",
    "print(\"a after masking=\")\n",
    "print(a)\n",
    "a = F.softmax(a, dim=-1)\n",
    "print(\"a after softmax=\")\n",
    "print(a)\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a @ b\n",
    "\n",
    "\n",
    "\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"c=\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 3: Using softmax\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T)) # weights\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow = wei @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "# We have 4 batches with 8 tokens each and each token has 32 channels, i.e. is encoded by 32 features\n",
    "B, T, C = 4, 8, 32 # batch, time, channels\n",
    "x = torch.randn((B,T,C))\n",
    "\n",
    "# simple average (weighted sum) of all past tokens and the current token. Previous information and current information are fused together using a simple average\n",
    "tril = torch.tril(torch.ones(T,T)) # initialize seperate lower triangular mask\n",
    "wei = torch.zeros((T,T)) # initializing these affinities / weights as 0 leads to the simply average (since all values are uniform)\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\")) # fill with -inf\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's implement actual data-dependent affinities ugin so-called Heads\n",
    "# so that we can move on from a simple average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "# We have 4 batches with 8 tokens each and each token has 32 channels, i.e. is encoded by 32 features\n",
    "B, T, C = 4, 8, 32 # batch, time, channels\n",
    "x = torch.randn((B,T,C))\n",
    "\n",
    "# simple average (weighted sum) of all past tokens and the current token. Previous information and current information are fused together using a simple average\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False).to(x.device)\n",
    "query = nn.Linear(C, head_size, bias=False).to(x.device)\n",
    "k = key(x)      # (B, T, 16)\n",
    "q = query(x)    # (B, T, 16)\n",
    "# --------- up to this point, no communication between the tokens has happened!!!\n",
    "# The communication comes now\n",
    "wei = q @ k.transpose(-2,-1)    # (B,T,16) @ (B,16,T) -> (B,T,T) --> square matrix of affinities\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T)) # initialize seperate lower triangular mask\n",
    "# wei = torch.zeros((T,T)) # initializing these affinities / weights as 0 leads to the simply average (since all values are uniform)\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\")) # fill with -inf\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
       "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
       "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
       "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
       "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
       "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
       "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, every single batch will have individualized / data-dependent weights based on its tokens and their affinities to each other\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]\n",
    "# let's investigate this.\n",
    "# below, we see the weights for one batch with 8 tokens.\n",
    "# How to read this?\n",
    "# In the first row, we investigate the first token.\n",
    "# And the first token is only allowed to know about past tokens and itself - so in this first case, only itself.\n",
    "# Therefore, it aggregates information from all previous tokens and itself. And of course, with only one token, its affinity is 1 (since it knows everything).\n",
    "# In the second row, we investigate the affinities for the second token. It has a low affinity to the first token and a high affinity to itself.\n",
    "# In the third row, we see that the third token has a somewhat equal affinity to the first and second token and the highest affinity to itself.\n",
    "# Moving along, for the eigth row, we can see that the eigth token has a high affinity to the fourth and seventh token as well as to itself.\n",
    "\n",
    "\n",
    "# NOW FOR SOME INTUITION, WHAT IS HAPPENING HERE?\n",
    "# If I am a token, I know about myself / my own content as well as my position in the sequence.\n",
    "# Therefore, I will embed this information into my own embedding as Key (K) and use it to compute affinities with other tokens.\n",
    "# So for example, if I am the eigth token in a sequence, and I am a vowel, I might look for consonants in the first four positions (that is my Query or Q).\n",
    "# So if, in the first four positions, there exists a consonant, then it in turn will have encoded this information (I am a consonant in position idx = 3) into its Key (K).\n",
    "# And by using the dot product matrix multiplication, I will match Q of the eight token with K from the third token and since the key has what I am looking for in my Query, it will produce a high affinity (in the below case 0.2297).\n",
    "\n",
    "# And to come back to the reason for maskin: It simply limits my query to keys that have come before me since I am not allowed to loook into the future.\n",
    "\n",
    "# So in short, the softmax normalized weight matrix below tells us how much information to aggregate from each individual prior token.\n",
    "# Tokens with high affinity will contribute proportionally more to the aggregated result than tokens with low affinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last step: include Value V besides Query Q and Key K\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "# We have 4 batches with 8 tokens each and each token has 32 channels, i.e. is encoded by 32 features\n",
    "B, T, C = 4, 8, 32 # batch, time, channels\n",
    "x = torch.randn((B,T,C))\n",
    "\n",
    "# simple average (weighted sum) of all past tokens and the current token. Previous information and current information are fused together using a simple average\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False).to(x.device)\n",
    "query = nn.Linear(C, head_size, bias=False).to(x.device)\n",
    "value  = nn.Linear(C, head_size, bias=False).to(x.device)\n",
    "\n",
    "k = key(x)      # (B, T, 16)\n",
    "q = query(x)    # (B, T, 16)\n",
    "# --------- up to this point, no communication between the tokens has happened!!!\n",
    "# The communication comes now\n",
    "wei = q @ k.transpose(-2,-1)    # (B,T,16) @ (B,16,T) -> (B,T,T) --> square matrix of affinities\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T)) # initialize seperate lower triangular mask\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))  / head_size**0.5# fill with -inf\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "v = value(x)     # (B, T, 16)\n",
    "out = wei @ v # THIS IS NEW\n",
    "\n",
    "out.shape\n",
    "\n",
    "# Think about x as the PRIVATE information of a token.\n",
    "# And v as the PUBLIC information.\n",
    "# Therefore, we have the following:\n",
    "# Query Q is what I am looking for (i.e. a consonant in positions 0 to 3)\n",
    "# Key K is what I am offering (i.e. I am a vowel in position idx = 7)\n",
    "# And v is the information that I contribute (i.e. I am the character \"t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important note: Masking is only necessary in a so-called decoder block (when we are trying to decode language and generate new tokens)\n",
    "# If, for example, we are interested in sentiment analysis of a sequence, we want the relations between all tokens in the sequence, i.e. forward and backward looking. In this case, we would use what is called an encoder block. And the only difference is, that we omit the masking part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-attention is called SELF-attention because the Q, K and Vs all come from the same source (in this case x)\n",
    "# In Encoder-Decoder Structures, cross-attention is used since we have seperate sources that we want to pull and combine data from.\n",
    "# In Cross-attention, the encoder produces Q, K and V values and the decoder produces Q, K and V values and those can match up with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
